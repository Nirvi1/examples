{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPuoZ4YUYK4o20RcUpJGNxG"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"5lUtUDVyGfyj","executionInfo":{"status":"ok","timestamp":1613185082539,"user_tz":-480,"elapsed":3624,"user":{"displayName":"Nirvi badyal","photoUrl":"","userId":"00762592465743850318"}}},"source":["import os\r\n","from io import open\r\n","import torch\r\n","print(\" here DATA \")\r\n","class Dictionary(object):\r\n","    def __init__(self):\r\n","        self.word2idx = {}\r\n","        self.idx2word = []\r\n","\r\n","    def add_word(self, word):\r\n","        if word not in self.word2idx:\r\n","            self.idx2word.append(word)\r\n","            self.word2idx[word] = len(self.idx2word) - 1\r\n","        return self.word2idx[word]\r\n","\r\n","    def __len__(self):\r\n","        return len(self.idx2word)\r\n","\r\n","\r\n","class Corpus(object):\r\n","    def __init__(self, path):\r\n","        self.dictionary = Dictionary()\r\n","        self.train = self.tokenize(os.path.join(path, 'train.txt'))\r\n","        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\r\n","        self.test = self.tokenize(os.path.join(path, 'test.txt'))\r\n","\r\n","    def tokenize(self, path):\r\n","        \"\"\"Tokenizes a text file.\"\"\"\r\n","        assert os.path.exists(path)\r\n","        # Add words to the dictionary\r\n","        with open(path, 'r', encoding=\"utf8\") as f:\r\n","            for line in f:\r\n","                words = line.split() + ['<eos>']\r\n","                for word in words:\r\n","                    self.dictionary.add_word(word)\r\n","\r\n","        # Tokenize file content\r\n","        with open(path, 'r', encoding=\"utf8\") as f:\r\n","            idss = []\r\n","            for line in f:\r\n","                words = line.split() + ['<eos>']\r\n","                ids = []\r\n","                for word in words:\r\n","                    ids.append(self.dictionary.word2idx[word])\r\n","                idss.append(torch.tensor(ids).type(torch.int64))\r\n","            ids = torch.cat(idss)\r\n","\r\n","        return ids\r\n"],"execution_count":1,"outputs":[]}]}